"""
Production-Ready ML Module Integration Tests

This module provides REAL integration tests for the ML module using:
- Real PostgreSQL database connections
- Real Redis cache connections
- Real MLService instances
- Real feature engineering
- Real model training and persistence
- Real prediction pipelines

NO MOCKS - All services use actual database connections and real implementations.
These tests verify production-ready integration patterns.
"""

import asyncio
import uuid
from datetime import datetime, timezone
from decimal import Decimal

import numpy as np
import pandas as pd
import pytest
import pytest_asyncio

from src.core.config import get_config
from src.core.types import MarketData
from src.ml.feature_engineering import FeatureEngineeringService, FeatureRequest
from src.ml.service import MLService, MLPipelineRequest, MLTrainingRequest


@pytest_asyncio.fixture
async def real_ml_service(di_container, clean_database):
    """Create and manage a real MLService for testing."""
    # Get ML service and dependencies from container
    ml_service = di_container.resolve("MLService")

    # Start dependent services first (they need to be running for MLService to use them)
    model_cache_service = di_container.resolve("ModelCacheService")
    model_registry_service = di_container.resolve("ModelRegistryService")
    inference_service = di_container.resolve("InferenceService")
    feature_engineering_service = di_container.resolve("FeatureEngineeringService")

    started_services = []

    try:
        # Start services in dependency order
        await model_cache_service.start()
        started_services.append(model_cache_service)

        await model_registry_service.start()
        started_services.append(model_registry_service)

        await feature_engineering_service.start()
        started_services.append(feature_engineering_service)

        await inference_service.start()
        started_services.append(inference_service)

        # Now start ML service
        await ml_service.start()
        started_services.append(ml_service)
        assert ml_service.is_running

        yield ml_service

    finally:
        # Cleanup in reverse order
        for service in reversed(started_services):
            try:
                await service.stop()
            except Exception as e:
                print(f"Error stopping {service.name}: {e}")


@pytest_asyncio.fixture
async def real_feature_engineering_service(di_container, clean_database):
    """Create and manage a real FeatureEngineeringService for testing."""
    # Get feature engineering service from container (already registered via di_container fixture)
    feature_service = di_container.resolve("FeatureEngineeringService")

    try:
        # Start feature engineering service
        await feature_service.start()
        assert feature_service.is_running

        yield feature_service

    finally:
        # Cleanup
        try:
            await feature_service.stop()
        except Exception as e:
            print(f"Error stopping feature engineering service: {e}")


@pytest_asyncio.fixture
def sample_market_data():
    """Create sample market data for testing."""
    # Create 100 data points with realistic market data
    timestamps = pd.date_range(start='2024-01-01', periods=100, freq='1h')

    # Generate realistic price movements
    np.random.seed(42)
    base_price = Decimal('50000.0')
    price_changes = np.random.normal(0, 100, 100).cumsum()

    data = []
    for i, ts in enumerate(timestamps):
        price = base_price + Decimal(str(price_changes[i]))
        data.append({
            'timestamp': ts,
            'symbol': 'BTC/USD',
            'open': price - Decimal('50.0'),
            'high': price + Decimal('100.0'),
            'low': price - Decimal('100.0'),
            'close': price,
            'volume': Decimal(str(np.random.uniform(10, 100)))
        })

    return data


@pytest_asyncio.fixture
def sample_training_data():
    """Create sample training data for ML models."""
    # Create features
    np.random.seed(42)
    n_samples = 200

    features = pd.DataFrame({
        'price_change': np.random.normal(0, 1, n_samples),
        'volume_change': np.random.normal(0, 0.5, n_samples),
        'volatility': np.random.uniform(0.01, 0.05, n_samples),
        'momentum': np.random.normal(0, 2, n_samples),
        'trend_strength': np.random.uniform(-1, 1, n_samples),
    })

    # Create target (binary classification: price up/down)
    target = (features['price_change'] + features['momentum'] > 0).astype(int)

    return features, target


@pytest.mark.integration
class TestRealFeatureEngineeringIntegration:
    """Real feature engineering integration tests with actual database connections."""

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_compute_price_features(self, real_feature_engineering_service, sample_market_data):
        """Test computing price-based features from market data."""
        request = FeatureRequest(
            market_data=sample_market_data,
            symbol='BTC/USD',
            feature_types=['price_features'],
            enable_preprocessing=False
        )

        response = await real_feature_engineering_service.compute_features(request)

        assert response.error is None
        assert response.feature_set is not None
        assert len(response.feature_set.features) > 0
        assert response.computation_time_ms > 0

        # Verify price features were created
        feature_names = response.feature_set.feature_names
        assert 'price_range' in feature_names
        assert 'price_change' in feature_names
        assert 'price_change_pct' in feature_names

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_compute_technical_indicators(self, real_feature_engineering_service, sample_market_data):
        """Test computing technical indicator features."""
        request = FeatureRequest(
            market_data=sample_market_data,
            symbol='BTC/USD',
            feature_types=['technical_indicators'],
            enable_preprocessing=False
        )

        response = await real_feature_engineering_service.compute_features(request)

        assert response.error is None
        assert response.feature_set is not None
        assert len(response.feature_set.features) > 0

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_compute_volume_features(self, real_feature_engineering_service, sample_market_data):
        """Test computing volume-based features."""
        request = FeatureRequest(
            market_data=sample_market_data,
            symbol='BTC/USD',
            feature_types=['volume_features'],
            enable_preprocessing=False
        )

        response = await real_feature_engineering_service.compute_features(request)

        assert response.error is None
        assert response.feature_set is not None
        assert len(response.feature_set.features) > 0

        # Verify volume features
        feature_names = response.feature_set.feature_names
        assert any('volume' in name for name in feature_names)

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_compute_volatility_features(self, real_feature_engineering_service, sample_market_data):
        """Test computing volatility features."""
        request = FeatureRequest(
            market_data=sample_market_data,
            symbol='BTC/USD',
            feature_types=['volatility_features'],
            enable_preprocessing=False
        )

        response = await real_feature_engineering_service.compute_features(request)

        assert response.error is None
        assert response.feature_set is not None
        assert len(response.feature_set.features) > 0

        # Verify volatility features
        feature_names = response.feature_set.feature_names
        assert any('volatility' in name or 'atr' in name for name in feature_names)

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_compute_all_feature_types(self, real_feature_engineering_service, sample_market_data):
        """Test computing all feature types together."""
        request = FeatureRequest(
            market_data=sample_market_data,
            symbol='BTC/USD',
            feature_types=[
                'price_features',
                'volume_features',
                'volatility_features',
                'momentum_features',
                'trend_features'
            ],
            enable_preprocessing=True,
            scaling_method='standard'
        )

        response = await real_feature_engineering_service.compute_features(request)

        assert response.error is None
        assert response.feature_set is not None
        assert len(response.feature_set.features) > 0
        assert len(response.feature_set.feature_names) > 10  # Should have many features
        assert response.preprocessing_info.get('scaling_method') == 'standard'

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_feature_selection(self, real_feature_engineering_service, sample_training_data):
        """Test feature selection functionality."""
        features_df, target_series = sample_training_data

        selected_df, selected_names, importance = await real_feature_engineering_service.select_features(
            features_df=features_df,
            target_series=target_series,
            method='mutual_info',
            max_features=3
        )

        assert len(selected_names) == 3
        assert len(selected_df.columns) == 3
        assert len(importance) == 3
        assert all(isinstance(score, Decimal) for score in importance.values())


@pytest.mark.integration
class TestRealMLServiceIntegration:
    """Real ML service integration tests with actual database connections."""

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_ml_service_initialization(self, real_ml_service):
        """Test ML service initializes with all dependencies."""
        assert real_ml_service.is_running

        # Verify service dependencies are resolved
        assert real_ml_service.ml_data_service is not None
        assert real_ml_service.ml_validation_service is not None
        assert real_ml_service.ml_integration_service is not None

        # Check health
        health = await real_ml_service.health_check()
        assert health is not None

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_ml_service_metrics(self, real_ml_service):
        """Test ML service metrics reporting."""
        metrics = real_ml_service.get_ml_service_metrics()

        assert 'active_operations' in metrics
        assert 'cache_manager_available' in metrics
        assert 'max_concurrent_operations' in metrics
        assert 'services_available' in metrics

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_train_simple_model(self, real_ml_service, sample_training_data):
        """Test training a simple ML model."""
        features_df, target_series = sample_training_data

        training_request = MLTrainingRequest(
            training_data=features_df.to_dict('records'),
            target_data=target_series.tolist(),
            model_type='random_forest',
            model_name='test_rf_model',
            hyperparameters={'n_estimators': 10, 'max_depth': 3},
            validation_split=0.2,
            cross_validation_folds=3,
            stage='development'
        )

        response = await real_ml_service.train_model(training_request)

        assert response.success
        assert response.error is None
        assert response.model_id is not None
        assert response.training_time_ms > 0
        assert 'accuracy' in response.training_metrics or 'mae' in response.training_metrics
        assert 'accuracy' in response.validation_metrics or 'mae' in response.validation_metrics

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_ml_pipeline_processing(self, real_ml_service, sample_market_data):
        """Test complete ML pipeline from data to prediction."""
        # First train a model
        # Create simple training data from market data
        df = pd.DataFrame(sample_market_data[:50])
        features = df[['open', 'high', 'low', 'close', 'volume']].copy()
        # Simple target: 1 if close > open, 0 otherwise
        target = (df['close'] > df['open']).astype(int)

        training_request = MLTrainingRequest(
            training_data=features.to_dict('records'),
            target_data=target.tolist(),
            model_type='random_forest',
            model_name='pipeline_test_model',
            hyperparameters={'n_estimators': 5, 'max_depth': 2},
            validation_split=0.3,
            feature_types=['price_features'],
            stage='development'
        )

        training_response = await real_ml_service.train_model(training_request)
        assert training_response.success

        # Now test pipeline processing with new data
        pipeline_request = MLPipelineRequest(
            symbol='BTC/USD',
            market_data=sample_market_data[50:60],
            model_id=training_response.model_id,
            feature_types=['price_features'],
            return_probabilities=False,
            use_cache=False
        )

        pipeline_response = await real_ml_service.process_pipeline(pipeline_request)

        assert pipeline_response.pipeline_success
        assert pipeline_response.error is None
        assert len(pipeline_response.predictions) > 0
        assert pipeline_response.model_id == training_response.model_id
        assert pipeline_response.total_processing_time_ms > 0
        assert 'feature_engineering' in pipeline_response.processing_stages

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_batch_pipeline_processing(self, real_ml_service, sample_market_data):
        """Test batch processing of ML pipelines."""
        # Create multiple pipeline requests
        requests = [
            MLPipelineRequest(
                symbol='BTC/USD',
                market_data=sample_market_data[i:i+10],
                model_name='default_model',
                feature_types=['price_features'],
                use_cache=False
            )
            for i in range(0, 30, 10)
        ]

        responses = await real_ml_service.process_batch_pipeline(requests)

        assert len(responses) == len(requests)
        # Some responses might fail (no model registered), but batch should complete
        assert all(isinstance(r, type(responses[0])) for r in responses)

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_model_persistence_and_loading(self, real_ml_service, sample_training_data):
        """Test that trained models are persisted and can be loaded."""
        features_df, target_series = sample_training_data

        # Train and save a model
        training_request = MLTrainingRequest(
            training_data=features_df.to_dict('records'),
            target_data=target_series.tolist(),
            model_type='logistic_regression',
            model_name='persistence_test_model',
            hyperparameters={'max_iter': 100},
            stage='development'
        )

        training_response = await real_ml_service.train_model(training_request)
        assert training_response.success
        model_id = training_response.model_id

        # List available models
        models = await real_ml_service.list_available_models(stage='development')

        # Should find our model
        model_found = any(m.get('model_id') == model_id for m in models)
        assert model_found

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_prediction_accuracy_validation(self, real_ml_service, sample_training_data):
        """Test that predictions are validated for accuracy."""
        features_df, target_series = sample_training_data

        # Split data for training and testing
        train_size = int(len(features_df) * 0.7)
        train_features = features_df[:train_size]
        train_target = target_series[:train_size]

        # Train model
        training_request = MLTrainingRequest(
            training_data=train_features.to_dict('records'),
            target_data=train_target.tolist(),
            model_type='random_forest',
            model_name='accuracy_test_model',
            hyperparameters={'n_estimators': 20, 'max_depth': 5},
            validation_split=0.2,
            stage='development'
        )

        response = await real_ml_service.train_model(training_request)

        # Verify training metrics are present
        assert response.training_metrics is not None
        assert response.validation_metrics is not None

        # Verify cross-validation was performed
        assert 'cv_score_mean' in response.validation_metrics


@pytest.mark.integration
class TestRealMLCrossModuleIntegration:
    """Test ML module integration with other modules."""

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_ml_with_real_market_data_integration(self, real_ml_service, clean_database):
        """Test ML service working with real market data from data module."""
        # Create realistic market data using MarketData type
        market_data_list = []
        base_price = Decimal('50000.0')

        for i in range(50):
            price = base_price + Decimal(str(i * 100))
            market_data_list.append({
                'timestamp': datetime.now(timezone.utc),
                'symbol': 'ETH/USD',
                'open': price - Decimal('10.0'),
                'high': price + Decimal('20.0'),
                'low': price - Decimal('20.0'),
                'close': price,
                'volume': Decimal('100.0')
            })

        # Test feature engineering on this data
        request = FeatureRequest(
            market_data=market_data_list,
            symbol='ETH/USD',
            feature_types=['price_features', 'volume_features'],
            enable_preprocessing=True
        )

        # This would go through ML service's feature engineering
        if real_ml_service.feature_engineering_service:
            response = await real_ml_service.feature_engineering_service.compute_features(request)
            assert response.error is None
            assert len(response.feature_set.features) > 0

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_ml_data_pipeline_integration(self, real_ml_service, sample_market_data):
        """Test ML data pipeline integration for preparing training data."""
        # Convert market data to DataFrame
        df = pd.DataFrame(sample_market_data)

        # Create features and target
        features = df[['open', 'high', 'low', 'close', 'volume']].copy()
        target = (df['close'].shift(-1) > df['close']).fillna(0).astype(int)

        # Train a model using the pipeline
        training_request = MLTrainingRequest(
            training_data=features[:-1].to_dict('records'),  # Exclude last row
            target_data=target[:-1].tolist(),
            model_type='random_forest',
            model_name='data_pipeline_model',
            hyperparameters={'n_estimators': 10},
            validation_split=0.2,
            stage='development'
        )

        response = await real_ml_service.train_model(training_request)
        assert response.success

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_ml_strategy_signal_enhancement(self, real_ml_service, sample_market_data):
        """Test ML service enhancing strategy signals."""
        # Create mock signals
        from decimal import Decimal

        class MockSignal:
            def __init__(self, symbol, strength):
                self.symbol = symbol
                self.strength = Decimal(str(strength))

        signals = [
            MockSignal('BTC/USD', 0.6),
            MockSignal('ETH/USD', 0.7),
            MockSignal('BTC/USD', 0.5)
        ]

        market_context = {
            'current_volatility': 0.02,
            'market_trend': 'bullish'
        }

        # Enhance signals using ML
        try:
            enhanced_signals = await real_ml_service.enhance_strategy_signals(
                strategy_id='test_strategy',
                signals=signals,
                market_context=market_context
            )

            # Should return same number of signals
            assert len(enhanced_signals) == len(signals)

        except Exception as e:
            # Enhancement might fail if no model is available, which is ok for this test
            assert 'model' in str(e).lower() or 'not found' in str(e).lower()

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_ml_cache_integration(self, real_ml_service):
        """Test ML service cache integration."""
        # Clear cache first
        cache_result = await real_ml_service.clear_cache()
        assert 'predictions_cleared' in cache_result

        # Test that cache metrics are available
        metrics = real_ml_service.get_ml_service_metrics()
        assert 'cache_manager_available' in metrics


@pytest.mark.integration
class TestRealMLErrorHandling:
    """Test ML service error handling and edge cases."""

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_invalid_training_data(self, real_ml_service):
        """Test handling of invalid training data."""
        # Empty training data
        training_request = MLTrainingRequest(
            training_data=[],
            target_data=[],
            model_type='random_forest',
            model_name='invalid_model',
            stage='development'
        )

        response = await real_ml_service.train_model(training_request)

        # Should handle gracefully
        assert not response.success
        assert response.error is not None

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_mismatched_data_lengths(self, real_ml_service):
        """Test handling of mismatched training data and target lengths."""
        training_request = MLTrainingRequest(
            training_data=[{'feature1': 1.0}] * 10,
            target_data=[1] * 5,  # Different length
            model_type='random_forest',
            model_name='mismatch_model',
            stage='development'
        )

        response = await real_ml_service.train_model(training_request)

        # Should handle gracefully
        assert not response.success
        assert response.error is not None

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_invalid_feature_type(self, real_feature_engineering_service, sample_market_data):
        """Test handling of invalid feature types."""
        request = FeatureRequest(
            market_data=sample_market_data,
            symbol='BTC/USD',
            feature_types=['invalid_feature_type'],
            enable_preprocessing=False
        )

        response = await real_feature_engineering_service.compute_features(request)

        # Should complete but may have warnings
        assert response.feature_set is not None

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_missing_market_data_fields(self, real_feature_engineering_service):
        """Test handling of market data with missing required fields."""
        incomplete_data = [
            {'timestamp': datetime.now(timezone.utc), 'symbol': 'BTC/USD'}
            # Missing price and volume fields
        ]

        request = FeatureRequest(
            market_data=incomplete_data,
            symbol='BTC/USD',
            feature_types=['price_features'],
            enable_preprocessing=False
        )

        response = await real_feature_engineering_service.compute_features(request)

        # Should handle missing data gracefully
        assert response is not None


@pytest.mark.integration
class TestRealMLPerformance:
    """Test ML service performance characteristics."""

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_concurrent_feature_computation(self, real_feature_engineering_service, sample_market_data):
        """Test concurrent feature computation requests."""
        requests = [
            FeatureRequest(
                market_data=sample_market_data,
                symbol=f'SYMBOL_{i}',
                feature_types=['price_features'],
                enable_preprocessing=False
            )
            for i in range(5)
        ]

        # Process concurrently
        tasks = [
            real_feature_engineering_service.compute_features(req)
            for req in requests
        ]

        responses = await asyncio.gather(*tasks)

        assert len(responses) == 5
        assert all(r.error is None for r in responses)

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_large_dataset_training(self, real_ml_service):
        """Test training with larger dataset."""
        # Generate larger dataset
        np.random.seed(42)
        n_samples = 1000

        features = pd.DataFrame({
            f'feature_{i}': np.random.randn(n_samples)
            for i in range(10)
        })
        target = (features['feature_0'] + features['feature_1'] > 0).astype(int)

        training_request = MLTrainingRequest(
            training_data=features.to_dict('records'),
            target_data=target.tolist(),
            model_type='random_forest',
            model_name='large_dataset_model',
            hyperparameters={'n_estimators': 20},
            validation_split=0.2,
            stage='development'
        )

        response = await real_ml_service.train_model(training_request)

        assert response.success
        assert response.training_time_ms > 0
        print(f"Training 1000 samples took {response.training_time_ms}ms")

    @pytest.mark.asyncio
    @pytest.mark.timeout(30)
    async def test_feature_computation_caching(self, real_feature_engineering_service, sample_market_data):
        """Test that feature computation uses caching effectively."""
        request = FeatureRequest(
            market_data=sample_market_data,
            symbol='CACHE_TEST',
            feature_types=['price_features'],
            enable_preprocessing=True
        )

        # First computation
        response1 = await real_feature_engineering_service.compute_features(request)
        time1 = response1.computation_time_ms

        # Second computation (should use cache)
        response2 = await real_feature_engineering_service.compute_features(request)
        time2 = response2.computation_time_ms

        # Both should succeed
        assert response1.error is None
        assert response2.error is None

        # Cache hit should be faster or similar
        # (In practice, cache might not always be faster due to serialization overhead)
        print(f"First: {time1}ms, Second (cached): {time2}ms")
